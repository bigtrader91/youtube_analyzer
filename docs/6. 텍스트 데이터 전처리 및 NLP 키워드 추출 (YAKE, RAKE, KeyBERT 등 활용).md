
### **개발 문서 6: 텍스트 데이터 전처리 및 NLP 키워드 추출 (YAKE, RAKE, KeyBERT 등 활용)**

**1. 모듈 목표:**

*   이전 단계들(개발 문서 4, 5)에서 추출한 유튜브 동영상 스크립트 및 댓글 텍스트 데이터를 NLP 분석에 적합하도록 전처리(Preprocessing).
*   전처리된 텍스트 데이터에 다양한 오픈소스 NLP 라이브러리(YAKE, RAKE-NLTK, KeyBERT 등)를 적용하여 핵심 키워드 및 키프레이즈(Keyphrases)를 추출.
*   추출된 키워드를 기반으로 콘텐츠 아이디어 발굴, SEO 최적화, 시청자 관심사 파악 등에 활용할 수 있는 결과 제공.

**2. 핵심 라이브러리:**

*   **텍스트 전처리:**
    *   `re`: 정규 표현식을 이용한 불필요한 문자 제거 (특수문자, 이모지, URL 등).
    *   `nltk`: 자연어 처리 툴킷. 토큰화(Tokenization), 불용어(Stopwords) 제거 등에 사용.
    *   `konlpy` (한국어 처리 시 선택): 한국어 형태소 분석 및 명사 추출 등에 사용. (설치가 다소 복잡할 수 있음)
    *   `spacy`: 고급 자연어 처리 라이브러리. 토큰화, 품사 태깅(POS tagging), 개체명 인식(NER) 등에 활용 가능.
*   **키워드 추출:**
    *   `yake`: 통계적 특징 기반의 비지도 학습 키워드 추출기. 비교적 간단하고 다국어 지원.
    *   `rake-nltk`: RAKE(Rapid Automatic Keyword Extraction) 알고리즘의 NLTK 기반 구현. 구문 기반 키워드 추출.
    *   `keybert`: BERT 임베딩을 활용하여 텍스트와 의미적으로 유사한 키워드를 추출. 문맥 이해도가 높음. (GPU 환경에서 더 빠름)
    *   `pytextrank` (spaCy 확장): TextRank 알고리즘 기반 키워드 및 요약 추출. 그래프 기반 방식.

**3. 필수 입력:**

*   `text_data`: 분석할 텍스트 데이터. 스크립트 또는 댓글 텍스트 문자열, 또는 여러 텍스트를 포함하는 리스트. (e.g., `[script_text_1, comment_text_1, comment_text_2, ...]`)
*   (선택) `language`: 텍스트의 언어 코드 (e.g., 'ko', 'en'). 라이브러리 설정에 필요.
*   (선택) `top_n`: 추출할 상위 키워드 개수.

**4. 주요 구현 단계:**

*   **(1) 텍스트 전처리 함수 정의:**
    *   입력 텍스트에 대해 다음 작업 수행:
        *   소문자 변환 (영문의 경우).
        *   정규 표현식을 사용하여 URL, 이메일 주소, 해시태그, 맨션(@), 숫자, 특수문자, 이모지 등 불필요한 요소 제거 또는 대체.
        *   (한국어) `konlpy` 등을 이용한 형태소 분석 및 명사/동사 등 필요한 품사 추출.
        *   (영어) `nltk` 또는 `spacy`를 이용한 토큰화 및 불용어(stopwords) 제거. (한국어 불용어 사전도 필요시 직접 구축 또는 활용)
        *   (선택) 표제어 추출(Lemmatization) 또는 어간 추출(Stemming) 수행.
    *   전처리된 텍스트(토큰 리스트 또는 정제된 문자열) 반환.
*   **(2) 키워드 추출 함수 정의 (각 라이브러리별):**
    *   **YAKE:**
        *   `yake.KeywordExtractor` 객체 생성 (언어, 상위 N개, 중복 제거 등 설정).
        *   `extractor.extract_keywords(processed_text)` 호출.
        *   결과(키워드, 점수) 반환.
    *   **RAKE-NLTK:**
        *   `rake_nltk.Rake` 객체 생성 (불용어 사전, 구문 길이 등 설정).
        *   `rake.extract_keywords_from_text(processed_text)` 호출.
        *   `rake.get_ranked_phrases_with_scores()` 로 점수와 함께 키워드(구문) 추출.
    *   **KeyBERT:**
        *   `KeyBERT` 모델 로드 (사전 훈련된 BERT 모델 지정, e.g., 'bert-base-nli-mean-tokens' 또는 한국어 모델 'skt/kobert-base-v1').
        *   `model.extract_keywords(original_text, keyphrase_ngram_range, stop_words, top_n)` 호출. (KeyBERT는 전처리되지 않은 원문을 입력으로 받을 수도 있음).
        *   결과(키워드, 유사도 점수) 반환.
    *   **TextRank (pytextrank):**
        *   `spacy` 파이프라인에 `pytextrank` 추가.
        *   `nlp(processed_text)` 실행.
        *   `doc._.phrases` 에서 추출된 키프레이즈(점수 포함) 접근.
*   **(3) 통합 실행 로직:**
    *   입력 텍스트 데이터(스크립트, 댓글 모음 등)를 전처리 함수에 전달.
    *   전처리된 텍스트를 각 키워드 추출 함수에 전달.
    *   각 라이브러리에서 추출된 키워드와 점수를 수집.
*   **(4) 결과 취합 및 후처리:**
    *   다양한 알고리즘에서 추출된 키워드들을 취합.
    *   (선택) 중복 제거, 점수 기반 정렬, 특정 품사 필터링 등 후처리 수행.
    *   최종 키워드 목록을 구조화하여 반환 (e.g., `{'yake': [('키워드1', 0.8), ...], 'rake': [('키워드 구문', 15.0), ...], 'keybert': [('다른 키워드', 0.9), ...]}`)

**5. 코드 스니펫 예시 (핵심 로직 - 영어 기준):**

```python
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
# nltk.download('punkt') # 최초 실행 시 필요
# nltk.download('stopwords') # 최초 실행 시 필요

import yake
from rake_nltk import Rake
from keybert import KeyBERT
# import spacy
# import pytextrank # spaCy와 pytextrank 설치 및 모델 다운로드 필요

# --- 1. 텍스트 전처리 함수 (영어 예시) ---
stop_words_en = set(stopwords.words('english'))

def preprocess_text_en(text):
    """영문 텍스트 전처리 함수"""
    if not isinstance(text, str): # 입력 타입 체크
        return ""
    text = text.lower() # 소문자 변환
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # URL 제거
    text = re.sub(r'\@\w+|\#','', text) # 맨션, 해시태그 제거
    text = re.sub(r'[^\w\s]', '', text) # 특수문자 제거 (알파벳, 숫자, 공백 제외)
    text = re.sub(r'\d+', '', text) # 숫자 제거
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word not in stop_words_en and len(word) > 1]
    return " ".join(filtered_tokens) # 토큰화된 결과를 다시 문자열로 합쳐서 반환 (일부 라이브러리는 문자열 입력 선호)

# --- 2. 키워드 추출 함수들 ---
def extract_yake_keywords(text, language="en", top_n=10):
    """YAKE로 키워드 추출"""
    try:
        kw_extractor = yake.KeywordExtractor(lan=language, n=1, dedupLim=0.9, top=top_n, features=None)
        keywords = kw_extractor.extract_keywords(text)
        return keywords # [('keyword', score), ...]
    except Exception as e:
        print(f"YAKE 오류: {e}")
        return []

def extract_rake_keywords(text, top_n=10):
    """RAKE-NLTK로 키워드(구문) 추출"""
    try:
        r = Rake()
        r.extract_keywords_from_text(text)
        keywords = r.get_ranked_phrases_with_scores()
        return keywords[:top_n] # [(score, 'keyword phrase'), ...] - 순서 주의!
    except Exception as e:
        print(f"RAKE 오류: {e}")
        return []

def extract_keybert_keywords(original_text, top_n=10, model_name='all-MiniLM-L6-v2'):
    """KeyBERT로 키워드 추출"""
    try:
        kw_model = KeyBERT(model=model_name)
        # KeyBERT는 불용어 제거 등 일부 전처리를 내부적으로 수행 가능
        keywords = kw_model.extract_keywords(original_text,
                                             keyphrase_ngram_range=(1, 2), # 1~2 단어 구문 추출
                                             stop_words='english',        # 내부 불용어 사용
                                             use_maxsum=True,             # 결과 다양성 높이기
                                             nr_candidates=20,            # 후보 키워드 수
                                             top_n=top_n)
        return keywords # [('keyword', score), ...]
    except Exception as e:
        print(f"KeyBERT 오류 ({model_name}): {e}")
        # 모델 다운로드 오류 등 발생 가능
        return []

# --- 3. 통합 실행 로직 (예시) ---
if __name__ == "__main__":
    # 이전 단계에서 가져온 스크립트 또는 댓글 텍스트 (예시)
    sample_script = """
    Welcome back to the channel! Today, we're diving deep into Python programming,
    specifically focusing on data structures like lists and dictionaries.
    Understanding these fundamental concepts is crucial for any aspiring Python developer.
    We'll cover list comprehensions, dictionary methods, and common pitfalls.
    Make sure to subscribe for more Python tutorials and data science content!
    Check out the link in the description for the source code. #Python #DataScience
    Visit example.com for more info. Contact me@example.com
    This video got 1000 views! Amazing! 😄
    """
    sample_comments = [
        "Great tutorial! Really helped me understand list comprehensions.",
        "What about sets and tuples? Can you make a video on those?",
        "Thanks! I was struggling with dictionary methods.",
        "Good explanation, but maybe cover nested dictionaries next time?",
        "Awesome content! subscribed 👍",
    ]

    # 스크립트 분석
    print("--- 스크립트 분석 ---")
    processed_script = preprocess_text_en(sample_script)
    print(f"전처리된 스크립트: {processed_script[:200]}...") # 일부만 출력

    yake_script_keywords = extract_yake_keywords(processed_script)
    rake_script_keywords = extract_rake_keywords(processed_script)
    keybert_script_keywords = extract_keybert_keywords(sample_script) # KeyBERT는 원문 사용 가능

    print("\nYAKE 키워드:", yake_script_keywords)
    print("RAKE 키워드/구문:", rake_script_keywords)
    print("KeyBERT 키워드:", keybert_script_keywords)

    # 댓글 분석 (모든 댓글을 하나로 합쳐서 분석하거나 개별 분석 가능)
    print("\n--- 댓글 분석 (통합) ---")
    combined_comments = " ".join(sample_comments)
    processed_comments = preprocess_text_en(combined_comments)
    print(f"전처리된 댓글: {processed_comments[:200]}...")

    yake_comment_keywords = extract_yake_keywords(processed_comments)
    rake_comment_keywords = extract_rake_keywords(processed_comments)
    keybert_comment_keywords = extract_keybert_keywords(combined_comments)

    print("\nYAKE 키워드:", yake_comment_keywords)
    print("RAKE 키워드/구문:", rake_comment_keywords)
    print("KeyBERT 키워드:", keybert_comment_keywords)

    # --- 4. 결과 취합 및 활용 (예시) ---
    # 각 방법별 키워드를 종합하여 빈도 분석, 중요도 평가 등 수행 가능
    all_extracted_keywords = {
        'script_yake': yake_script_keywords,
        'script_rake': rake_script_keywords,
        'script_keybert': keybert_script_keywords,
        'comment_yake': yake_comment_keywords,
        'comment_rake': rake_comment_keywords,
        'comment_keybert': keybert_comment_keywords,
    }
    # 이 데이터를 바탕으로 콘텐츠 주제 선정, 태그 제안, 제목 아이디어 구상 등에 활용
    # 예를 들어, 댓글에서 자주 언급된 'sets', 'tuples', 'nested dictionaries'는 다음 영상 주제로 고려 가능
```

**6. 데이터 출력:**

*   모듈은 입력된 텍스트(스크립트, 댓글 등)에 대해 각 NLP 라이브러리가 추출한 키워드(또는 키프레이즈)와 해당 키워드의 중요도 점수(알고리즘별 상이)를 구조화된 형태로 반환. 보통 **딕셔너리 형태**로 각 알고리즘의 결과를 담아 반환하는 것이 유용.
*   이 결과는 사용자가 직접 검토하여 콘텐츠 아이디어를 얻거나, 다른 모듈과 연동하여 자동 태그 생성, 관련 키워드 확장 등의 기능 구현에 사용될 수 있음.

**7. 오류 처리 및 고려 사항:**

*   **라이브러리 설치:** NLP 라이브러리, 특히 `konlpy`나 `spaCy` 모델 등은 설치 과정이 다소 복잡하거나 의존성 문제가 발생할 수 있음. 가상 환경 사용을 강력히 권장.
*   **언어 지원:** 사용하는 라이브러리가 분석 대상 텍스트의 언어를 지원하는지 확인해야 함. 한국어의 경우 `konlpy`, 한국어 BERT 모델(KeyBERT용) 등이 필요. 전처리 단계(불용어 사전 등)도 해당 언어에 맞게 조정해야 함.
*   **전처리 중요성:** 키워드 추출의 품질은 텍스트 전처리 수준에 크게 영향을 받음. 분석 목적과 데이터 특성에 맞게 전처리 단계를 신중하게 설계해야 함. 너무 많은 정보를 제거하면 중요한 키워드를 놓칠 수 있고, 너무 적게 제거하면 노이즈가 많아짐.
*   **알고리즘 특성 이해:** 각 키워드 추출 알고리즘은 다른 원리로 작동함.
    *   YAKE/RAKE: 통계/규칙 기반으로 비교적 빠르지만 문맥 이해 부족.
    *   KeyBERT/TextRank: 임베딩/그래프 기반으로 의미론적 유사성이나 문맥을 고려하지만 계산 비용이 더 높을 수 있음.
    *   여러 알고리즘 결과를 비교하고 조합하는 것이 더 강건한 결과를 얻는 데 도움이 될 수 있음.
*   **성능:** 대량의 텍스트 데이터를 처리할 경우, 특히 KeyBERT와 같이 딥러닝 모델을 사용하는 경우 처리 시간이 오래 걸릴 수 있음. GPU 활용 또는 효율적인 코드 작성 고려.
*   **키워드 품질 평가:** 추출된 키워드가 실제 콘텐츠의 핵심 내용을 잘 반영하는지는 정성적인 평가가 필요함. 점수만으로 판단하기 어려울 수 있음.

**8. 종합 및 향후 확장 방향:**

*   지금까지 작성된 개발 문서 1~6을 통해 Google/YouTube API와 오픈소스 라이브러리를 활용하여 키워드 조사, 경쟁 분석, 콘텐츠 텍스트(스크립트, 댓글) 추출 및 핵심 키워드 도출까지의 파이프라인을 구축할 수 있음.
*   **향후 확장 방향:**
    *   **결과 시각화:** 추출된 키워드, 검색량 추이 등을 그래프나 워드 클라우드로 시각화.
    *   **감성 분석:** 댓글 텍스트에 감성 분석을 적용하여 시청자 반응(긍정/부정) 파악.
    *   **주제 모델링 (Topic Modeling):** LDA 등 주제 모델링 기법을 적용하여 대량의 텍스트 데이터에서 숨겨진 주제 구조 발견.
    *   **채널 분석 통합:** YouTube Analytics API를 연동하여 실제 채널 성과 데이터(조회수, 시청 시간, 구독자 변화 등)와 키워드/콘텐츠 분석 결과 비교.
    *   **썸네일 분석 자동화:** (주의: 기술적 어려움) 경쟁 영상 썸네일 이미지들을 수집하고, 이미지 분석 기술(색상 분석, 객체 탐지 등)을 적용하여 트렌드 파악 시도.
    *   **사용자 인터페이스:** Streamlit, Flask/Django 등을 사용하여 웹 기반 인터페이스 개발.

---

이 문서 시리즈는 데이터 기반 유튜브 성장을 위한 자동화 도구 개발의 핵심 구성 요소들을 다루었습니다. 실제 구현 시에는 각 단계의 세부적인 오류 처리, 사용자 입력 처리, 결과 저장 방식 등을 더욱 견고하게 설계해야 합니다.