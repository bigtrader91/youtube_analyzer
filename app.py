"""
YouTube ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ - Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
"""
import os
import time
import pandas as pd
import streamlit as st
from pathlib import Path

# ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸
from src.api_clients import initialize_all_clients
from src.youtube_searcher import search_and_get_details
from src.comment_extractor import get_video_comments, comments_to_dataframe
from src.transcript_extractor import extract_and_clean_transcripts
from src.nlp_processor import KeywordExtractor
from src.data_manager import DataManager, ReportGenerator

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="YouTube ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸ¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'initialized_apis' not in st.session_state:
    st.session_state.initialized_apis = False
if 'youtube_search_results' not in st.session_state:
    st.session_state.youtube_search_results = None
if 'selected_video_ids' not in st.session_state:
    st.session_state.selected_video_ids = []
if 'transcripts_data' not in st.session_state:
    st.session_state.transcripts_data = {}
if 'comments_data' not in st.session_state:
    st.session_state.comments_data = {}
if 'nlp_results' not in st.session_state:
    st.session_state.nlp_results = {}

# ë°ì´í„° ê´€ë¦¬ì ë° ë¦¬í¬íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
data_manager = DataManager()
report_generator = ReportGenerator(data_manager)

# ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.title("YouTube ë¶„ì„ ë„êµ¬")
    
    # API ì„¤ì • (ì‚¬ì´ë“œë°”)
    st.header("API ì„¤ì •")
    api_key_youtube = st.text_input("YouTube Data API í‚¤", type="password")
    
    # ë³´ì•ˆ ê²½ê³ 
    st.warning("ì´ ì…ë ¥ì€ ê°œë°œ ë° í…ŒìŠ¤íŠ¸ìš©ì…ë‹ˆë‹¤. ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” í™˜ê²½ ë³€ìˆ˜ë‚˜ Streamlit Secretsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # API ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("API ì´ˆê¸°í™”"):
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ì‹œ)
        os.environ["YOUTUBE_API_KEY"] = api_key_youtube
        
        with st.spinner("API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘..."):
            youtube_success, _ = initialize_all_clients()
            
            if youtube_success:
                st.session_state.initialized_apis = True
                st.success("YouTube API ì´ˆê¸°í™” ì„±ê³µ!")
            else:
                st.error("YouTube API ì´ˆê¸°í™” ì‹¤íŒ¨. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    # ê¸°ë³¸ ì„¤ì •
    st.header("ê¸°ë³¸ ì„¤ì •")
    language_code = st.selectbox(
        "ë¶„ì„ ì–¸ì–´",
        options=["ko", "en"],
        format_func=lambda x: "í•œêµ­ì–´" if x == "ko" else "ì˜ì–´",
        key="language_select"
    )
    
    # ì•± ì •ë³´
    st.header("ì•± ì •ë³´")
    st.info(
        """
        ì´ ì•±ì€ YouTube Data APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒì„ ê²€ìƒ‰í•˜ê³ , 
        ìŠ¤í¬ë¦½íŠ¸ì™€ ëŒ“ê¸€ì„ ì¶”ì¶œí•˜ì—¬ NLP ê¸°ë²•ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        """
    )

# ë©”ì¸ ì˜ì—­ êµ¬ì„±
st.title("YouTube ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ")

if not st.session_state.initialized_apis and not api_key_youtube:
    st.warning("ì‚¬ìš©ì„ ì‹œì‘í•˜ë ¤ë©´ ì‚¬ì´ë“œë°”ì— YouTube API í‚¤ë¥¼ ì…ë ¥í•˜ê³  ì´ˆê¸°í™”í•˜ì„¸ìš”.")

# íƒ­ ìƒì„±
tab1, tab2, tab3, tab4 = st.tabs([
    "YouTube ê²€ìƒ‰", 
    "ë™ì˜ìƒ ìƒì„¸ ë¶„ì„", 
    "NLP í‚¤ì›Œë“œ ì¶”ì¶œ",
    "ì¢…í•© ë¦¬í¬íŠ¸"
])

# íƒ­ 1: YouTube ê²€ìƒ‰
with tab1:
    st.header("YouTube ê²€ìƒ‰")
    
    search_query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", placeholder="ì˜ˆ: íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ê°•ì˜")
    col1, col2 = st.columns(2)
    
    with col1:
        max_search_results = st.slider("ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜", 5, 50, 10)
    
    with col2:
        content_type = st.radio("ê²€ìƒ‰ ìœ í˜•", ["video", "channel"], format_func=lambda x: "ë™ì˜ìƒ" if x == "video" else "ì±„ë„")
    
    if st.button("YouTube ê²€ìƒ‰ ì‹¤í–‰"):
        if not st.session_state.initialized_apis:
            st.error("APIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        elif not search_query:
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("YouTube ê²€ìƒ‰ ì¤‘..."):
                try:
                    results_df = search_and_get_details(
                        query=search_query,
                        max_results=max_search_results,
                        content_type=content_type,
                        include_details=True
                    )
                    
                    # ê²°ê³¼ ì €ì¥
                    if not results_df.empty:
                        data_manager.save_df_to_csv(
                            results_df,
                            f"search_results_{search_query.replace(' ', '_')}",
                            subdir="videos" if content_type == "video" else "channels"
                        )
                        st.session_state.youtube_search_results = results_df
                        st.success(f"YouTube ê²€ìƒ‰ ì™„ë£Œ! {len(results_df)}ê°œ ê²°ê³¼ ì°¾ìŒ.")
                    else:
                        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    if st.session_state.youtube_search_results is not None:
        st.subheader("ê²€ìƒ‰ ê²°ê³¼")
        
        df_display = st.session_state.youtube_search_results
        
        if content_type == "video":
            # ë™ì˜ìƒ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            display_cols = ['videoId', 'title', 'channelTitle', 'publishedAt', 'viewCount', 'likeCount', 'commentCount']
            display_cols = [col for col in display_cols if col in df_display.columns]
            
            # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
            st.dataframe(df_display[display_cols], use_container_width=True)
            
            # ë¶„ì„í•  ë™ì˜ìƒ ì„ íƒ
            video_ids_to_analyze = st.multiselect(
                "ë¶„ì„í•  ë™ì˜ìƒ ID ì„ íƒ:",
                options=df_display['videoId'].dropna().unique().tolist(),
                default=[],
                key="video_select"
            )
            
            # ì„ íƒëœ ë™ì˜ìƒ ì €ì¥
            if st.button("ì„ íƒí•œ ë™ì˜ìƒ ë¶„ì„ ì¤€ë¹„"):
                if video_ids_to_analyze:
                    st.session_state.selected_video_ids = video_ids_to_analyze
                    st.success(f"{len(video_ids_to_analyze)}ê°œ ë™ì˜ìƒì´ ë¶„ì„ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. 'ë™ì˜ìƒ ìƒì„¸ ë¶„ì„' íƒ­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                else:
                    st.warning("ë¶„ì„í•  ë™ì˜ìƒì„ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        else:
            # ì±„ë„ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            display_cols = ['channelId', 'title', 'description', 'publishedAt', 'subscriberCount', 'videoCount', 'viewCount']
            display_cols = [col for col in display_cols if col in df_display.columns]
            
            st.dataframe(df_display[display_cols], use_container_width=True)

# íƒ­ 2: ë™ì˜ìƒ ìƒì„¸ ë¶„ì„
with tab2:
    st.header("ë™ì˜ìƒ ìƒì„¸ ë¶„ì„ (ìŠ¤í¬ë¦½íŠ¸/ëŒ“ê¸€)")
    
    if not st.session_state.selected_video_ids:
        st.warning("YouTube ê²€ìƒ‰ íƒ­ì—ì„œ ë¨¼ì € ë¶„ì„í•  ë™ì˜ìƒì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        st.write("ì„ íƒëœ ë™ì˜ìƒ ID:")
        for video_id in st.session_state.selected_video_ids:
            st.code(video_id)
        
        col1, col2 = st.columns(2)
        
        with col1:
            fetch_comments = st.checkbox("ëŒ“ê¸€ ìˆ˜ì§‘", value=True)
            max_comments = st.slider("ìµœëŒ€ ëŒ“ê¸€ ìˆ˜", 10, 500, 100)
        
        with col2:
            fetch_transcript = st.checkbox("ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì§‘", value=True)
            preferred_languages = st.multiselect(
                "ì„ í˜¸ ì–¸ì–´ (ìŠ¤í¬ë¦½íŠ¸)",
                options=["ko", "en", "ja", "zh-Hans", "zh-Hant", "es", "fr", "de"],
                default=["ko", "en"],
                format_func=lambda x: {
                    "ko": "í•œêµ­ì–´", "en": "ì˜ì–´", "ja": "ì¼ë³¸ì–´", 
                    "zh-Hans": "ì¤‘êµ­ì–´(ê°„ì²´)", "zh-Hant": "ì¤‘êµ­ì–´(ë²ˆì²´)",
                    "es": "ìŠ¤í˜ì¸ì–´", "fr": "í”„ë‘ìŠ¤ì–´", "de": "ë…ì¼ì–´"
                }.get(x, x)
            )
        
        if st.button("ìŠ¤í¬ë¦½íŠ¸/ëŒ“ê¸€ ì¶”ì¶œ ì‹¤í–‰"):
            if not st.session_state.initialized_apis:
                st.error("APIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            else:
                all_transcripts = {}
                all_comments = {}
                
                for i, video_id in enumerate(st.session_state.selected_video_ids):
                    st.write(f"---\n**[{i+1}/{len(st.session_state.selected_video_ids)}] ë™ì˜ìƒ '{video_id}' ì²˜ë¦¬ ì¤‘...**")
                    
                    # ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
                    if fetch_transcript:
                        with st.spinner(f"ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ ì¤‘... ({video_id})"):
                            try:
                                transcript_df = extract_and_clean_transcripts(
                                    video_ids=[video_id],
                                    preferred_languages=preferred_languages
                                )
                                
                                if not transcript_df.empty:
                                    data_manager.save_df_to_csv(
                                        transcript_df,
                                        f"{video_id}_transcript",
                                        subdir="transcripts"
                                    )
                                    
                                    # ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                    transcript_text = ""
                                    if 'cleaned_transcript' in transcript_df.columns:
                                        transcript_text = transcript_df['cleaned_transcript'].iloc[0]
                                    elif 'raw_transcript' in transcript_df.columns:
                                        transcript_text = transcript_df['raw_transcript'].iloc[0]
                                    
                                    all_transcripts[video_id] = {
                                        'text': transcript_text,
                                        'status': transcript_df['status'].iloc[0],
                                        'char_count': len(transcript_text) if transcript_text else 0
                                    }
                                    
                                    if transcript_df['status'].iloc[0] == 'success':
                                        st.success(f"ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ ì„±ê³µ! (ê¸€ì ìˆ˜: {len(transcript_text)})")
                                    else:
                                        reason = transcript_df['error_reason'].iloc[0] if 'error_reason' in transcript_df.columns else 'ì•Œ ìˆ˜ ì—†ìŒ'
                                        st.warning(f"ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {reason}")
                                else:
                                    st.warning(f"ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({video_id})")
                                    all_transcripts[video_id] = {
                                        'text': "",
                                        'status': 'failed',
                                        'char_count': 0
                                    }
                            except Exception as e:
                                st.error(f"ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                                all_transcripts[video_id] = {
                                    'text': "",
                                    'status': 'error',
                                    'error': str(e),
                                    'char_count': 0
                                }
                    
                    # ëŒ“ê¸€ ìˆ˜ì§‘
                    if fetch_comments:
                        with st.spinner(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... ({video_id})"):
                            try:
                                comments = get_video_comments(
                                    video_id=video_id,
                                    max_total_comments=max_comments,
                                    fetch_replies=True
                                )
                                
                                if comments:
                                    comments_df = comments_to_dataframe(comments, include_replies=True)
                                    data_manager.save_df_to_csv(
                                        comments_df,
                                        f"{video_id}_comments",
                                        subdir="comments"
                                    )
                                    
                                    all_comments[video_id] = {
                                        'df': comments_df,
                                        'count': len(comments_df),
                                        'all_text': " ".join(comments_df['text'].dropna().astype(str).tolist())
                                    }
                                    
                                    st.success(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì„±ê³µ! (ì´ {len(comments_df)}ê°œ)")
                                else:
                                    st.warning(f"ëŒ“ê¸€ì´ ì—†ê±°ë‚˜ ìˆ˜ì§‘ ì‹¤íŒ¨. ({video_id})")
                                    all_comments[video_id] = {
                                        'df': pd.DataFrame(),
                                        'count': 0,
                                        'all_text': ""
                                    }
                            except Exception as e:
                                st.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                                all_comments[video_id] = {
                                    'df': pd.DataFrame(),
                                    'count': 0,
                                    'all_text': "",
                                    'error': str(e)
                                }
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.transcripts_data = all_transcripts
                st.session_state.comments_data = all_comments
                
                st.success(f"ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! ({len(st.session_state.selected_video_ids)}ê°œ ë™ì˜ìƒ)")
        
        # ìˆ˜ì§‘ëœ ë°ì´í„° í‘œì‹œ
        if st.session_state.transcripts_data or st.session_state.comments_data:
            st.subheader("ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            
            # ìŠ¤í¬ë¦½íŠ¸ ë¯¸ë¦¬ë³´ê¸°
            if st.session_state.transcripts_data:
                st.write("**ìŠ¤í¬ë¦½íŠ¸:**")
                for video_id, transcript in st.session_state.transcripts_data.items():
                    if transcript['status'] == 'success' and transcript['text']:
                        with st.expander(f"ë™ì˜ìƒ ID: {video_id} (ê¸€ì ìˆ˜: {transcript['char_count']})"):
                            st.text_area(
                                "ìŠ¤í¬ë¦½íŠ¸", 
                                transcript['text'][:1000] + ("..." if len(transcript['text']) > 1000 else ""),
                                height=150
                            )
            
            # ëŒ“ê¸€ ë¯¸ë¦¬ë³´ê¸°
            if st.session_state.comments_data:
                st.write("**ëŒ“ê¸€ (ìƒìœ„ 5ê°œ):**")
                for video_id, comment_data in st.session_state.comments_data.items():
                    if comment_data['count'] > 0:
                        with st.expander(f"ë™ì˜ìƒ ID: {video_id} (ëŒ“ê¸€ ìˆ˜: {comment_data['count']})"):
                            st.dataframe(comment_data['df'][['text', 'author', 'likeCount']].head())

# íƒ­ 3: NLP í‚¤ì›Œë“œ ì¶”ì¶œ
with tab3:
    st.header("NLP í‚¤ì›Œë“œ ì¶”ì¶œ")
    
    if not st.session_state.selected_video_ids:
        st.warning("YouTube ê²€ìƒ‰ íƒ­ì—ì„œ ë¨¼ì € ë¶„ì„í•  ë™ì˜ìƒì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    elif not st.session_state.transcripts_data and not st.session_state.comments_data:
        st.warning("'ë™ì˜ìƒ ìƒì„¸ ë¶„ì„' íƒ­ì—ì„œ ë¨¼ì € ìŠ¤í¬ë¦½íŠ¸ì™€ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
    else:
        col1, col2 = st.columns(2)
        
        with col1:
            text_source_option = st.radio(
                "ë¶„ì„í•  í…ìŠ¤íŠ¸ ì†ŒìŠ¤ ì„ íƒ:", 
                ["ìŠ¤í¬ë¦½íŠ¸", "ëŒ“ê¸€", "ìŠ¤í¬ë¦½íŠ¸+ëŒ“ê¸€"],
                key="text_source"
            )
        
        with col2:
            nlp_methods = st.multiselect(
                "ì‚¬ìš©í•  NLP ë°©ë²• ì„ íƒ:", 
                ["YAKE", "RAKE", "KeyBERT"],
                default=["YAKE", "RAKE"],
                format_func=lambda x: x,
                key="nlp_methods"
            )
        
        video_for_analysis = st.selectbox(
            "ë¶„ì„í•  ë™ì˜ìƒ ì„ íƒ:",
            st.session_state.selected_video_ids,
            key="video_for_analysis"
        )
        
        top_n = st.slider("ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜", 5, 50, 20)
        
        # ë¶„ì„ ë²„íŠ¼
        if st.button("NLP í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰"):
            if not video_for_analysis:
                st.error("ë¶„ì„í•  ë™ì˜ìƒì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            elif not nlp_methods:
                st.error("í•˜ë‚˜ ì´ìƒì˜ NLP ë°©ë²•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                # ì„ íƒëœ í…ìŠ¤íŠ¸ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
                text_to_process = ""
                source_name = ""
                
                if text_source_option == "ìŠ¤í¬ë¦½íŠ¸" and video_for_analysis in st.session_state.transcripts_data:
                    text_to_process = st.session_state.transcripts_data[video_for_analysis].get('text', "")
                    source_name = "ìŠ¤í¬ë¦½íŠ¸"
                
                elif text_source_option == "ëŒ“ê¸€" and video_for_analysis in st.session_state.comments_data:
                    text_to_process = st.session_state.comments_data[video_for_analysis].get('all_text', "")
                    source_name = "ëŒ“ê¸€"
                
                elif text_source_option == "ìŠ¤í¬ë¦½íŠ¸+ëŒ“ê¸€":
                    transcript_text = ""
                    comment_text = ""
                    
                    if video_for_analysis in st.session_state.transcripts_data:
                        transcript_text = st.session_state.transcripts_data[video_for_analysis].get('text', "")
                    
                    if video_for_analysis in st.session_state.comments_data:
                        comment_text = st.session_state.comments_data[video_for_analysis].get('all_text', "")
                    
                    text_to_process = transcript_text + " " + comment_text
                    source_name = "ìŠ¤í¬ë¦½íŠ¸+ëŒ“ê¸€"
                
                if not text_to_process.strip():
                    st.error(f"ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì„ íƒí•œ ì†ŒìŠ¤({text_source_option})ì— ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                else:
                    with st.spinner("NLP í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘..."):
                        try:
                            # KeywordExtractor ì´ˆê¸°í™”
                            language = st.session_state.get("language_select", "ko")
                            keyword_extractor = KeywordExtractor(language=language)
                            
                            # ë°©ë²•ëª… ë³€í™˜ (ëŒ€ë¬¸ì -> ì†Œë¬¸ì)
                            methods_to_use = [method.lower() for method in nlp_methods]
                            
                            # í‚¤ì›Œë“œ ì¶”ì¶œ
                            nlp_results = keyword_extractor.extract_keywords(
                                text_to_process,
                                methods=methods_to_use,
                                top_n=top_n,
                                preprocess=True
                            )
                            
                            # í†µí•© í‚¤ì›Œë“œ
                            combined_keywords = keyword_extractor.combine_keywords(
                                nlp_results,
                                top_n=top_n
                            )
                            
                            # ê²°ê³¼ ì €ì¥
                            st.session_state.nlp_results[video_for_analysis] = {
                                'source': source_name,
                                'methods': methods_to_use,
                                'results': nlp_results,
                                'combined': combined_keywords
                            }
                            
                            # ë¶„ì„ ê²°ê³¼ ì €ì¥
                            result_data = {
                                'video_id': video_for_analysis,
                                'text_source': source_name,
                                'text_length': len(text_to_process),
                                'keywords': nlp_results,
                                'combined_keywords': combined_keywords
                            }
                            
                            data_manager.save_dict_to_json(
                                result_data,
                                f"{video_for_analysis}_{source_name}_keywords",
                                subdir="analysis"
                            )
                            
                            st.success("NLP í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
                        except Exception as e:
                            st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ê²°ê³¼ í‘œì‹œ
        if video_for_analysis in st.session_state.nlp_results:
            result = st.session_state.nlp_results[video_for_analysis]
            st.subheader(f"í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ - {result['source']}")
            
            # í†µí•© í‚¤ì›Œë“œ í‘œì‹œ
            st.write("**í†µí•© í‚¤ì›Œë“œ (ìƒìœ„ ìˆœìœ„ìˆœ)**")
            
            # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
            df_combined = pd.DataFrame(result['combined'])
            
            # ë°ì´í„°í”„ë ˆì„ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€
            # ë°ì´í„° êµ¬ì¡° ë””ë²„ê¹…
            if df_combined.empty:
                st.warning("ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # ë””ë²„ê¹… ì •ë³´
                st.write("ë””ë²„ê¹… ì •ë³´: í˜„ì¬ ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼:", df_combined.columns.tolist())
                
                # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ì²˜ë¦¬
                required_columns = ['keyword', 'score', 'methods', 'count']
                missing_columns = [col for col in required_columns if col not in df_combined.columns]
                
                if missing_columns:
                    st.warning(f"ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆìŠµë‹ˆë‹¤: {missing_columns}")
                    # ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€
                    for col in missing_columns:
                        df_combined[col] = "N/A"
                
                # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ í‘œì‹œ
                existing_columns = [col for col in required_columns if col in df_combined.columns]
                if existing_columns:
                    df_combined = df_combined[existing_columns]
                    
                    # ì»¬ëŸ¼ëª… ë³€ê²½
                    column_mapping = {
                        'keyword': 'í‚¤ì›Œë“œ',
                        'score': 'ì ìˆ˜',
                        'methods': 'ì¶”ì¶œ ë°©ë²•',
                        'count': 'ë¹ˆë„'
                    }
                    renamed_columns = {col: column_mapping.get(col, col) for col in existing_columns}
                    df_combined.rename(columns=renamed_columns, inplace=True)
                else:
                    # ëª¨ë“  ì»¬ëŸ¼ í‘œì‹œ
                    pass
                    
                # í…Œì´ë¸” í‘œì‹œ
                st.dataframe(df_combined, use_container_width=True)
            
            # ê°œë³„ ë°©ë²•ë³„ ê²°ê³¼ í‘œì‹œ
            st.write("**ê°œë³„ ë°©ë²•ë³„ ê²°ê³¼**")
            
            tabs_methods = st.tabs([method.upper() for method in result['methods']])
            
            for i, method in enumerate(result['methods']):
                if method in result['results']:
                    with tabs_methods[i]:
                        keywords = result['results'][method]
                        df_method = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ì ìˆ˜'])
                        st.dataframe(df_method, use_container_width=True)
            
            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            if st.button("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"):
                with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                    try:
                        wc_path = report_generator.generate_wordcloud(
                            result['combined'],
                            f"{video_for_analysis} - {result['source']} í‚¤ì›Œë“œ",
                            f"{video_for_analysis}_{result['source'].replace('+', '_')}_wordcloud",
                            subdir="reports"
                        )
                        
                        if wc_path:
                            st.success("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì™„ë£Œ!")
                            st.image(wc_path)
                        else:
                            st.error("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨.")
                    except Exception as e:
                        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# íƒ­ 4: ì¢…í•© ë¦¬í¬íŠ¸
with tab4:
    st.header("ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±")
    
    if not st.session_state.selected_video_ids:
        st.warning("YouTube ê²€ìƒ‰ íƒ­ì—ì„œ ë¨¼ì € ë¶„ì„í•  ë™ì˜ìƒì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        # ë¦¬í¬íŠ¸ ìƒì„±í•  ë™ì˜ìƒ ì„ íƒ
        videos_for_report = st.multiselect(
            "ë¦¬í¬íŠ¸ ìƒì„±í•  ë™ì˜ìƒ ì„ íƒ:",
            st.session_state.selected_video_ids,
            default=st.session_state.selected_video_ids,
            key="videos_for_report"
        )
        
        include_wordcloud = st.checkbox("ì›Œë“œí´ë¼ìš°ë“œ í¬í•¨", value=True)
        
        if st.button("ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"):
            if not videos_for_report:
                st.error("ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ë™ì˜ìƒì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner(f"ì´ {len(videos_for_report)}ê°œ ë™ì˜ìƒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
                    try:
                        # ì¼ê´„ ë¶„ì„ ìˆ˜í–‰
                        batch_results = report_generator.generate_batch_analysis(
                            video_ids=videos_for_report,
                            include_wordclouds=include_wordcloud
                        )
                        
                        st.success(f"ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ! (ì„±ê³µ: {batch_results['successful_analyses']}, ì‹¤íŒ¨: {batch_results['failed_analyses']})")
                        
                        # ë¦¬í¬íŠ¸ ê²°ê³¼ í‘œì‹œ
                        for i, report_item in enumerate(batch_results['video_reports']):
                            video_id = report_item['video_id']
                            status = report_item['status']
                            
                            if status == 'success':
                                st.write(f"**{i+1}. {video_id}**: ì„±ê³µ âœ…")
                                
                                # ë¦¬í¬íŠ¸ íŒŒì¼ ë¡œë“œ
                                report_data = data_manager.load_dict_from_json(
                                    f"{video_id}_comprehensive_report",
                                    subdir="reports"
                                )
                                
                                if report_data:
                                    with st.expander(f"{video_id} ë¦¬í¬íŠ¸ ìƒì„¸ë³´ê¸°"):
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            st.write("**ìŠ¤í¬ë¦½íŠ¸ í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)**")
                                            if 'transcript_keywords' in report_data and report_data['transcript_keywords']:
                                                for j, kw in enumerate(report_data['transcript_keywords'][:10], 1):
                                                    st.write(f"{j}. {kw['keyword']} (ì ìˆ˜: {kw['score']:.2f})")
                                            else:
                                                st.write("ìŠ¤í¬ë¦½íŠ¸ í‚¤ì›Œë“œ ì—†ìŒ")
                                        
                                        with col2:
                                            st.write("**ëŒ“ê¸€ í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)**")
                                            if 'comment_keywords' in report_data and report_data['comment_keywords']:
                                                for j, kw in enumerate(report_data['comment_keywords'][:10], 1):
                                                    st.write(f"{j}. {kw['keyword']} (ì ìˆ˜: {kw['score']:.2f})")
                                            else:
                                                st.write("ëŒ“ê¸€ í‚¤ì›Œë“œ ì—†ìŒ")
                                        
                                        # ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ í‘œì‹œ
                                        if include_wordcloud:
                                            if 'script_wordcloud_path' in report_data:
                                                st.write("**ìŠ¤í¬ë¦½íŠ¸ ì›Œë“œí´ë¼ìš°ë“œ**")
                                                st.image(report_data['script_wordcloud_path'])
                                            
                                            if 'comments_wordcloud_path' in report_data:
                                                st.write("**ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ**")
                                                st.image(report_data['comments_wordcloud_path'])
                            else:
                                reason = report_item.get('reason', 'ì•Œ ìˆ˜ ì—†ìŒ')
                                st.write(f"**{i+1}. {video_id}**: ì‹¤íŒ¨ âŒ - {reason}")
                    except Exception as e:
                        st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# í‘¸í„°
st.markdown("---")
st.markdown(
    """
    <div style="text-align: center;">
        <p>YouTube ì½˜í…ì¸  ë¶„ì„ ì‹œìŠ¤í…œ - Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤</p>
        <p>ê°œë°œ: YouTube Analyzer í”„ë¡œì íŠ¸</p>
    </div>
    """,
    unsafe_allow_html=True
) 